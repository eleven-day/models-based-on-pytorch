# 基于pytorch实现的transformer架构代码

## Word_Embedding
这段代码中定义的类Embeddings是一个词嵌入模型, 主要用于将文本数据中的每个单词映射到一个高维的向量，以便在深度学习模型中使用。这个类继承自PyTorch的nn.Module类，可作为深度学习模型中的一部分。

初始化函数__init__包含两个参数，分别是词嵌入向量的维数d_model和词汇表的大小vocab。然后在初始化函数中创建一个名为self.lut的嵌入层，该层可以将输入的每个单词映射到一个d_model维的向量。

在前向传播函数forward中，输入x被送到嵌入层self.lut中，并得到嵌入向量。然后，这些嵌入向量会被缩放，即乘以嵌入向量的维数d_model的平方根。若干个缩放操作可以帮助控制嵌入向量的大小，使得其在计算后续模型的损失函数时更加稳定。

## Positional_Encoding
这段代码是用于实现位置编码（Positional Encoding）的，位置编码是一种在自然语言处理（NLP）任务中应用的编码技术，尤其在 Transformer 算法中的应用较为广泛。其目的是向模型提供单词在句子中的位置信息。由于 transformer 等模型的输入是无序的，所以需要通过这种方式提供位置信息。位置编码采用一对正弦函数和余弦函数来为每个位置生成一个独特的编码，通过这种方式，模型就能够区分出序列中不同位置的单词。

## Self_Attention
这段代码定义了一个名为ScaledDotProductAttention的类，这是用于处理下一层输入的注意力机制的一种常用方式——缩放点积注意力。这个机制是Transformer模型中关键的组成部分。

这种机制的目的是决定网络的每一层在处理输入时对于不同输入元素的专注程度。简而言之，该模型在处理输入数据时，会给不同的输入元素分配不同的“关注权重”，这就是所谓的“注意力”。

实际操作中，它通过 q（Query）和 k（Keys）的矩阵乘法来计算注意力，并在此后对结果进行缩放，然后用 softmax 函数处理这些得分。对得分执行了 dropout 操作以预防过拟合后，将处理过的注意力与 v（Values）相乘以得到最终输出。

## Multi_Head_Attention
该段代码是实现多头注意力机制（Multi-Head Attention）的代码，多头注意力机制是在自注意力模型中用于提取不同位置不同深度信息的重要部分。每一“头”都有各自的查询、键、值权重，通过这种方式，网络可以关注输入的不同部分，这样有助于模型在理解文本的多重语义方面做得更好。

## Add_Layer
这段代码定义了一种计算层次标准化（Layer Normalization）的类LayerNorm。层次标准化是一种常用的深度学习模型中的正则化技术，可以加速模型训练，提高模型的泛化能力。这个类中，gamma和beta是该层的可学习参数，前向传播中，用输入减去其均值，再除以其标准差实现标准化，然后乘以gamma并加上beta实现重标定，输出与输入有相同的shape，但数值已经标准化。

## Feed_Forward
这段代码定义了一个位置逐点前馈网络（Position-wise Feed Forward Network），这是 Transformer 模型的重要组成部分。它是由两层完全连接的前馈神经网络组成，并在输入和输出之间有残差连接，并经过层标准化处理。此网络广泛用于自然语言处理和其他相关的机器学习任务。

## EncoderLayer
这段代码定义了一个Transformer模型中的编码层类 EncoderLayer，该类包括一个多头自注意力子层（MultiHeadAttention）和一个位置前馈网络子层（PoswiseFeedForwardNet）。输入数据首先会通过多头注意力子层进行自注意力操作，再通过位置前馈网络进行处理，最后输出处理结果。

## Encoder
这段代码定义了一个编码器类，其中使用了词嵌入层（nn.Embedding）、位置编码层（PositionalEncoding）、编码层（EncoderLayer）和获取注意力掩码的函数（get_attn_pad_mask）。在前向传播方法中，首先通过词嵌入层和位置编码层，得到编码输入的嵌入和位置编码，然后通过自注意力掩码，随后进入多层编码层进行编码。这类编码器主要用于神经网络模型Transformer中的编码器部分。

## DecoderLayer
这段代码定义了一个解码器层的类，其中使用了多头自注意力（MultiHeadAttention）和位置前馈神经网络（PoswiseFeedForwardNet）。在前向传播方法中，先经过多头自注意力操作，将解码器的输入进行自注意力计算，然后进行编码-解码注意力计算，最后通过位置前馈网络操作。主要用于神经网络模型Transformer中的解码器部分。

## Decoder
这段代码实现了Transformer模型的解码器部分。其主要功能是接收编码器的输出和目标序列，通过多层解码器层完成信息解码，输出最后的结果。在每一层中，都会有两种注意力机制：自注意力机制处理解码器自身的信息，并通过注意力mask做到解码时的自回归性质；编码器-解码器注意力机制则处理编码器的输出和解码器的信息，并通过相应的mask保证了注意力计算的正确性。其中的位置嵌入则通过一个预先计算好的Sin/Cos函数的位置编码表完成，这使得模型可以获得序列中的位置信息。

## Transformer
这段代码的核心功能是定义和实现了一个Transformer模型，这个模型主要包含了一个编码器(Encoder)、一个解码器(Decoder)以及一个输出层，这个输出层是用来进行词汇预测的。在模型的前向传播中，会先通过编码器对输入进行编码，并输出编码结果和自注意力图(enc_self_attns)，再通过解码器对编码结果进行解码，并输出解码结果、解码器的自注意力图(dec_self_attns)以及解码器对编码结果的注意力图(dec_enc_attns)。最后，将解码结果通过输出层进行词汇预测。

## attention is all you need
attention is all you need-comparison论文的中英文对照版，attention is all you need-zh论文的中文翻译版。

